{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strange-volunteer",
   "metadata": {},
   "source": [
    "# Perceptroni Multi-Strato\n",
    "\n",
    "I _Perceptroni Multi-Strato_ (_Multi-Layer Perceptron_, **MLP**) sono l'esempio tipico di _Rete Neurale_ (_Neural Network:, **NN**).\n",
    "\n",
    "## MLP in Breve\n",
    "\n",
    "Gli MLP sono un particolare tipo di _feedforward_ NN (cioè senza ricorsività/cicli nella sua struttura) caratterizzato da una sequenza di _strati completamente connessi_ (_fully-connected layers_, *FC Layers*).\n",
    "\n",
    "------------------------------\n",
    "**\"DEFINIZIONE\"** _(Funzione caratterizzante un Fully-Connected Layer)_**:**\n",
    "\n",
    "Sia $L$ un FC layer di $m\\in\\mathbb{N}$ unità con funzione di attivazione $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ e sia $L$ (completamente) connesso con un altro livello di $n$ unità. Il layer $L$ è quindi caratterizzato dalla funzione $\\mathcal{L}:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ tale che\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\boldsymbol{x}) := \\boldsymbol{\\sigma}\\left(W\\boldsymbol{x} + \\boldsymbol{b}\\right)\\,,\\quad \\forall \\ \\boldsymbol{x}\\in\\mathbb{R}^n\\,,\n",
    "\\end{equation}\n",
    "dove:\n",
    "- $W\\in\\mathbb{R}^{m\\times n}$ è la matrice dei pesi del livello $L$;\n",
    "- $\\boldsymbol{b}\\in\\mathbb{R}^m$ è il vettore dei bias\n",
    "- $\\boldsymbol{\\sigma}:\\mathbb{R}^m\\rightarrow\\mathbb{R}^m$ è una funzione vettoriale che applica elemento-per-elemento la funzione $\\sigma$.\n",
    "------------------------------\n",
    "\n",
    "------------------------------\n",
    "**\"DEFINIZIONE\"** _(Input Layer)_**:**\n",
    "\n",
    "Un _Input Layer_ di $n\\in\\mathbb{N}$ unità è un layer che \"_legge_\" vettori di $\\mathbb{R}^n$ e li \"_invia_\" ai layer successivi con lui connessi.\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-values",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "**\"DEFINIZIONE\"** _(Funzione caratterizzante un Multi-Layer Perceptron)_**:**\n",
    "\n",
    "Sia dato un MLP costituito da un input layer $L^{(0)}$ di $n\\in\\mathbb{N}$ unità, seguito da una sequenza di FC layers $L^{(1)},\\ldots ,L^{(H)}, L^{(H+1)}$ connessi uno dopo l'altro. In particolare, i layer $L^{(1)},\\ldots ,L^{(H)}$ sono definiti _strati nascosti_ (_hidden layers_) mentre $L^{(H+1)}$ è definito _strato di output_ (_output layer_).\n",
    "\n",
    "L'MLP in questione è quindi rappresentato da una funzione $\\hat{\\boldsymbol{F}}:\\mathbb{R}^n\\rightarrow \\mathbb{R}^m$ tale che\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{F}}(\\boldsymbol{x}) : \\mathbb{R}^n \\xrightarrow[]{\\mathcal{L}^{(1)}} \\mathbb{R}^{n_1} \\xrightarrow[]{\\mathcal{L}^{(2)}} \\cdots \\xrightarrow[]{\\mathcal{L}^{(H)}} \\mathbb{R}^{n_H}\\xrightarrow[]{\\mathcal{L}^{(H+1)}}\\mathbb{R}^m\n",
    "\\end{equation}\n",
    "\n",
    "e in particolare \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{F}}(\\boldsymbol{x}) = \\boldsymbol{\\sigma}^{(H+1)}\\left( W^{(H+1)}\\boldsymbol{\\sigma}^{(H)}\\left(\\cdots \\left( W^{(2)}\\boldsymbol{\\sigma}^{(1)}\\left(W^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right)+\\boldsymbol{b}^2\\right) \\cdots\\right)  + \\boldsymbol{b}^{(H+1)}\\right)\\,,\\quad \\forall \\ \\boldsymbol{x}\\in\\mathbb{R}^n\\,,\n",
    "\\end{equation}\n",
    "\n",
    "dove:\n",
    "- $W^{(h)}$, $\\boldsymbol{b}^{(h)}$, $\\sigma^{(h)}$, sono rispettivamente i pesi, i bias e la funzione di attivazione del layer $h$-esimo, per ogni $h=1,\\ldots , H+1$;\n",
    "- $n_h\\in\\mathbb{N}$ è il numero di unità del layer $h$-esimo, per ogni $h=1,\\ldots ,H$;\n",
    "- $m\\in\\mathbb{N}$ è il numero di unità dell'output layer $L^{(H+1)}$.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "-----------------\n",
    "\n",
    "**OSSERVAZIONE** _($\\hat{\\boldsymbol{F}}$ come Funzione Parametrica)_**:** \n",
    "\n",
    "La funzione $\\hat{\\boldsymbol{F}}$ caratterizzante un MLP è, come le funzioni di tutti gli altri algoritmi di Machine Learning (ML) una funzione parametrica con parametri i pesi $W^{(h)}$ ed i bias $\\boldsymbol{b}^{(h)}$. Indicando con $\\boldsymbol{w}$ il vettore ottenuto dalla concatenazione delle vettorializzazioni di tutti i pesi $W^{(h)}$ e di tutti i bias $\\boldsymbol{b}^{(h)}$, possiamo esplicitare la dipendenza dell'MLP da questi parametri con la seguente notazione:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{F}}(\\ \\cdot \\ \\,;\\, \\boldsymbol{w})\\quad \\text{oppure} \\quad \\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}(\\ \\cdot \\ )\n",
    "\\end{equation}\n",
    "\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-richards",
   "metadata": {},
   "source": [
    "## Metodi di Discesa del Gradiente\n",
    "\n",
    "\n",
    "I metodi di discesa del gradiente sono _metodi numerici iterativi_ per la minimizzazione (massimizzazione) di funzioni $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$. \n",
    "\n",
    "Le funzioni da minimizzare sono generalmente definite indifferentemente come \"_funzioni obiettivo_\" (_objective functions_), \"_funzioni di costo_\" (_cost functions_) o \"_funzioni di perdita_\" (_loss functions_).\n",
    "\n",
    "Questi metodi si basano sull'osservazione che $-\\nabla f(\\boldsymbol{x}_0)$ è la _direzione di più ripida discesa_ per $f$ nel punto $\\boldsymbol{x}_0$ (analogamente, $\\nabla f(\\boldsymbol{x}_0)$ è la direzione di più ripida ascesa).\n",
    "\n",
    "------------------\n",
    "\n",
    "**DEFINIZIONE** _(Direzione di Discesa)_**:**\n",
    "\n",
    "Un vettore $\\boldsymbol{p}\\in\\mathbb{R}^n$ è una _direzione di discesa_ per la funzione $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ in $\\boldsymbol{x}_0\\in\\mathbb{R}^n$ se esiste $\\alpha^*\\in\\mathbb{R}^+$ tale che\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\boldsymbol{x}_0)\\geq f(\\boldsymbol{x}_0 + \\alpha\\boldsymbol{p})\\,,\\quad \\forall \\ \\alpha\\in [0, \\alpha^*]\n",
    "\\end{equation}\n",
    "\n",
    "------------------\n",
    "\n",
    "------------------\n",
    "\n",
    "**METODO** _(Steepest Descent - a grandi linee...)_**:**\n",
    "\n",
    "Il _metodo di più ripida discesa_ (_steepest descent method_) è un metodo di discesa del gradiente per la minimizzazione di funzioni $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$, cioè per trovare la soluzione al problema\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\boldsymbol{x}\\in\\mathbb{R}^n} f(\\boldsymbol{x})\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Dato quindi un punto di partenza _arbitrario_ $\\boldsymbol{x}_0\\in\\mathbb{R}^n$, abbiamo che il passo $k$-esimo del metodo è definito dalla seguente operazione:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_{k} - \\alpha_{k}\\nabla f(\\boldsymbol{x}_k)\\,, \\quad \\forall \\ k\\geq 0\\,,\n",
    "\\end{equation}\n",
    "\n",
    "con $\\alpha_k\\in\\mathbb{R}^+$ è un _fattore di moltiplicazione del passo di discesa_ (nei casi più semplici, $\\alpha_k=\\alpha$ costante per ogni $k$).\n",
    "\n",
    "\n",
    "------------------\n",
    "\n",
    "------------------\n",
    "\n",
    "**LAVORARE CON IL METODO DI PIU' RIPIDA DISCESA** _(Proprietà e Problemi)_**:**\n",
    "\n",
    "1. Sotto specifiche ipotesi di regolarità per $f$ e/o opportune scelte di $\\boldsymbol{x}_0$ ed $\\{\\alpha_k\\}_{k\\in\\mathbb{N}}$, il metodo _CONVERGE_ ad un minimo _LOCALE_ della funzione;\n",
    "\n",
    "2. Se non si hanno informazioni sulle proprietà di $f$, la convergenza del metodo è altamente influenzata dalla scelta del punto di partenza $\\boldsymbol{x}_0$ e dalla successione $\\{\\alpha_k\\}_{k\\in\\mathbb{N}}$ di moltiplicatori del passo. Il metodo potrebbe quindi CONVERGERE, DIVERGERE, \"OSCILLLARE\";\n",
    "\n",
    "3. Vanno stabiliti opportuni criteri di arresto (altrimenti il metodo contrinuerebbe all'infinito). Anche i parametri caratterizzanti i criteri di arresto influenzano la convergenza del metodo.\n",
    "\n",
    "------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-azerbaijan",
   "metadata": {},
   "source": [
    "\n",
    "## Addestramento di un MLP - in Breve\n",
    "\n",
    "L'addestramento di un MLP (e di una NN in generale) richiede sia conoscenze teoriche dei metodi di ottimizzazione numerica che una certa sensibilità pratica/empirica per la scelta dei parametri che li caratterizzano. \n",
    "\n",
    "------------------\n",
    "**ATTENZIONE** _(Aggiunta di una Componente Stocastica)_**:** \n",
    "\n",
    "Si deve inoltre considerare che l'addestramento di NN introduce delle componenti _stocastiche_ nei metodi numerici sopra citati. In altre parole, non si minimizza una generica funzione di costo \"_fissata_\", ma una funzione di costo caratterizzata dai campioni del mio dataset che uso per l'addestramento.\n",
    "\n",
    "------------------\n",
    "\n",
    "### Funzioni di Loss Rispetto a \"Gruppi\" di Dati\n",
    "\n",
    "\n",
    "Data una funzione target $\\boldsymbol{F}$, per addestrare un MLP con funzione parametrica $\\hat{\\boldsymbol{F}}_\\boldsymbol{w}$ ad apprendere $\\boldsymbol{F}$, vorremmo _idealmente_ risolvere il problema\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\boldsymbol{w}} \\left\\lbrace \\mathrm{Loss}(\\boldsymbol{w}):= \\sum_{\\boldsymbol{x}\\in\\mathbb{R}^n}|\\boldsymbol{F}(\\boldsymbol{x}) - \\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}(\\boldsymbol{x})|\\right\\rbrace\\,;\n",
    "\\end{equation}\n",
    "in altre parole, vorremmo trovare $\\boldsymbol{w}^*$ tale che $\\mathrm{Loss}(\\boldsymbol{w}^*)=0$.\n",
    "\n",
    "**ATTENZIONE:** NOTARE CHE NEL PROBLEMA DI MINIMO LE VARIABILI SONO I PARAMETRI $\\boldsymbol{w}$, NON LE $\\boldsymbol{x}$!!\n",
    "\n",
    "Ovviamente, non disponendo degli infiniti punti $\\boldsymbol{x}\\in\\mathbb{R}^n$ (e spesso neanche della funzione target $\\boldsymbol{F}$, ma solo delle valutazioni $\\boldsymbol{y}=\\boldsymbol{F}(\\boldsymbol{x})$), il problema ideale sopra indicato va ri-adattato alla pratica, cioè ai dati disponibili contenuti nel training set\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{T}=\\{ (\\boldsymbol{x}_1,\\boldsymbol{y}_1),\\ldots ,  (\\boldsymbol{x}_T,\\boldsymbol{y}_T) \\}\\,.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-facing",
   "metadata": {},
   "source": [
    "#### Approccio 1: Niente Stocasticità\n",
    "\n",
    "L'idea più semplice per addestrare un MLP può essere quindi quello di definire una funzione di loss \"troncando\" quella del problema ideale, cioè:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):=\\sum_{i=1}^T \\underbrace{|\\boldsymbol{y}_i - \\hat{\\boldsymbol{F}}(\\boldsymbol{x}_i)| }_{\\text{err. el.-per-el.}} \\,.\n",
    "\\end{equation}\n",
    "\n",
    "In questo caso, il problema di minimizzazione vede una loss \"fissata\" e può essere risolto tramite il metodo di più ripida discesa (o sue varianti); avremo quindi che l'addestramento del mio MLP sarà così definito:\n",
    "\n",
    "-------------------\n",
    "**SCHEMA** _(Addestramento - Approccio 1)_**:**\n",
    "\n",
    "L'algoritmo di addestramento per questo approccio è esemplificato dai seguenti passi:\n",
    "\n",
    "1. **Inizializzazione Pesi:** genero dei pesi $\\boldsymbol{w}$ e li assegno all'architettura del mio MLP (ottengo quindi $\\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}$);\n",
    "2. **Finché** non viene raggiunto un criterio di arresto (vedremo in seguito), ripetere:\n",
    "    1. **Aggiornamento Pesi:** $\\boldsymbol{w}\\gets \\boldsymbol{w} - \\alpha \\nabla \\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w})$;\n",
    "    \n",
    "**Definizione:** ogni ripetizione dei passi del punto (2) è definito _EPOCA DI ADDESTRAMENTO_, nella terminologia delle NN. Il più semplice dei criteri di arresto è quindi quello di inserire un numero massimo di epoche da eseguire.\n",
    "\n",
    "**Definizione:** il parametro $\\alpha$, nella terminologia delle NN, è definito _tasso di apprendimento_ (_learning rate_). Anche se non esplicito nella notazione utilizzata sopra, $\\alpha$ può variare da epoca ad epoca. \n",
    "  \n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-aquatic",
   "metadata": {},
   "source": [
    "#### Approccio 2: Stocasticità \"Pura\"\n",
    "\n",
    "Poiché addestrare l'MLP rispetto ad un errore \"aggregato\" rispetto a tutto il training set potrebbe dare risultati poco sensibili ai dettagli della funzione target $\\boldsymbol{F}$, l'idea è quella di introdurre della stocasticità nel metodo di ottimizzazione (prendiamo come riferimento sempre la _steepest descent_).\n",
    "\n",
    "Definiamo quindi una funzione di loss \"_parametrica_\", i cui parametri sono una singola coppia di $\\mathcal{T}$, cioè:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Loss}_{(\\boldsymbol{x},\\boldsymbol{y})}(\\boldsymbol{w}):= |\\boldsymbol{y} - \\hat{\\boldsymbol{F}}(\\boldsymbol{x})| \\,;\n",
    "\\end{equation}\n",
    "\n",
    "Data questa loss, il metodo cambia \"spezzando\" il passo (A) dell'_Approccio 1_ in tanti sottopassi, uno per ogni coppia in $\\mathcal{T}$\n",
    "\n",
    "In questo caso, il problema di minimizzazione vede una loss \"variabile\"; avremo quindi che l'addestramento del mio MLP sarà così definito:\n",
    "\n",
    "-------------------\n",
    "**SCHEMA** _(Addestramento - Approccio 2)_**:**\n",
    "\n",
    "L'algoritmo di addestramento per questo approccio è esemplificato dai seguenti passi:\n",
    "\n",
    "1. **Inizializzazione Pesi:** genero dei pesi $\\boldsymbol{w}$ e li assegno all'architettura del mio MLP (ottengo quindi $\\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}$);\n",
    "2. **Finché** non viene raggiunto un criterio di arresto (vedremo in seguito), ripetere:\n",
    "    1. **\"Mescolo\" le coppie in** $\\mathcal{T}$;\n",
    "    2. **Per ogni** $(\\boldsymbol{x}, \\boldsymbol{y})$ in $\\mathcal{T}$ \"mescolato\":\n",
    "        1. **Aggiornamento Pesi:** $\\boldsymbol{w}\\gets \\boldsymbol{w} - \\alpha \\nabla \\mathrm{Loss}_{(\\boldsymbol{x},\\boldsymbol{y})}(\\boldsymbol{w})$;\n",
    "    \n",
    "**Definizione:** ogni ripetizione dei passi del punto (2) è sempre definito _EPOCA DI ADDESTRAMENTO_, come per l'_Approccio 1_.\n",
    "\n",
    "**Definizione:** il parametro $\\alpha$, è sempre il _tasso di apprendimento_ (_learning rate_). Anche se non esplicito nella notazione utilizzata sopra, $\\alpha$ può variare da epoca ad epoca. \n",
    "\n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-fortune",
   "metadata": {},
   "source": [
    "#### Approccio 3: Metodi Mini-Batch\n",
    "\n",
    "L'approccio di fatto più utilizzato nella pratica è una via di mezzo tra il primo ed il terzo (ed in realtà i primi due approcci sono casi particolari di questo).\n",
    "\n",
    "Continuiamo a definire una funzione di loss \"_parametrica_\", i cui parametri non sono una singola coppia di $\\mathcal{T}$, ma un suo arbitrario sottoinsieme $\\mathcal{B}$ (chiamato _minibatch_), cioè:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Loss}_{\\,\\mathcal{B}}(\\boldsymbol{w}):= \\sum_{(\\boldsymbol{x}, \\boldsymbol{y})\\in\\mathcal{B}}|\\boldsymbol{y} - \\hat{\\boldsymbol{F}}(\\boldsymbol{x})| \\,;\n",
    "\\end{equation}\n",
    "\n",
    "Data questa loss, il metodo cambia \"spezzando\" il passo (A) dell'_Approccio 1_ in tanti sottopassi, analogamente all'_Approccio 2_, uno per ogni sottoinsieme $\\mathcal{B}$ estratto.\n",
    "\n",
    "In questo caso, il problema di minimizzazione vede una loss \"variabile\"; avremo quindi che l'addestramento del mio MLP sarà così definito:\n",
    "\n",
    "-------------------\n",
    "**SCHEMA** _(Addestramento - Approccio 3)_**:**\n",
    "\n",
    "Fissato un numero $K\\in\\mathbb{N}$ di minibatch in cui \"spezzare\" $\\mathcal{T}$ (generalmente si fa secondo una cardinalità comune dei minibatch), l'algoritmo di addestramento per questo approccio è esemplificato dai seguenti passi:\n",
    "\n",
    "1. **Inizializzazione Pesi:** genero dei pesi $\\boldsymbol{w}$ e li assegno all'architettura del mio MLP (ottengo quindi $\\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}$);\n",
    "2. **Finché** non viene raggiunto un criterio di arresto (vedremo in seguito), ripetere:\n",
    "    1. **Genero Casualmente** una $K$-partizione in minibatch _distinti_ $\\mathcal{B}_1,\\ldots , \\mathcal{B}_K$ di $\\mathcal{T}$;\n",
    "    2. **Per ogni** $k=1,\\ldots , K$:\n",
    "        1. **Aggiornamento Pesi:** $\\boldsymbol{w}\\gets \\boldsymbol{w} - \\alpha \\nabla \\mathrm{Loss}_{\\,\\mathcal{B}_k}(\\boldsymbol{w})$;\n",
    "    \n",
    "**Definizione:** ogni ripetizione dei passi del punto (2) è sempre definito _EPOCA DI ADDESTRAMENTO_, come per gli _Approcci 1_ e _2_.\n",
    "\n",
    "**Definizione:** il parametro $\\alpha$, è sempre il _tasso di apprendimento_ (_learning rate_). Anche se non esplicito nella notazione utilizzata sopra, $\\alpha$ può variare da epoca ad epoca. \n",
    "  \n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-crystal",
   "metadata": {},
   "source": [
    "#### Loss Notevoli\n",
    "\n",
    "A seconda della funzione $\\boldsymbol{F}$ da apprendere, la loss su $\\mathcal{T}$, $(\\boldsymbol{x}, \\boldsymbol{y})$ o $\\mathcal{B}$ può anche essere definita in maniera differente, scegliendo un _errore elemento-per-elemento_ o un _aggregatore_ differenti. La loss sopra considerata è una _somma di errori assoluti_, ma altre equivalenti possono essere definite, per esempio:\n",
    "1. _Media di Errori Quadratici_ (_Mean Square Error_, **MSE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):=\\frac{1}{T}\\sum_{i=1}^{T} \\left(\\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) \\right)^2$;\n",
    "2. _Media di Errori Assoluti_ (_Mean Absolute Error_, **MAE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):=\\frac{1}{T}\\sum_{i=1}^{T} | \\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) |$;\n",
    "3. _Somma di Errori Quadratici_ (_Sum of Square Error_, **MSE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):= \\sum_{i=1}^{T} \\left(\\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) \\right)^2$;\n",
    "4. _Somma di Errori Assoluti_ (_Sum of Absolute Error_, **MAE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):= \\sum_{i=1}^{T} | \\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) |$.\n",
    "\n",
    "Le loss sopra indicate sono utili per problemi di _regressione_ e per la spiegazione della teoria. Per problemi di classificazione si utilizza generalmente altre loss (p.e. la _cross-entropy loss_) e/o particolari output layer (_softmax layers_). Non li tratteremo per concentrarci sulle proprietà generali dell'addestramento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-curtis",
   "metadata": {},
   "source": [
    "## Criteri di Arresto\n",
    "\n",
    "Come anticipato nelle scorse esercitazioni, per le NN il validation set $\\mathcal{V}$ svolge un ruolo generalmente differente rispetto ai classici algoritmi di ML.\n",
    "\n",
    "Per capire meglio, bisogna richiamare due fenomeni che possono avvenire in caso di _cattivo addestramento_ della NN:\n",
    "1. **Underfitting:** la NN si è addestrata male su $\\mathcal{T}$ ed ha quindi cattive performance sia su di esso che sul test set $\\mathcal{P}$.\n",
    "2. **Overfitting:** la NN si è addestrata \"_troppo_\" su $\\mathcal{T}$. Ha quindi imparato molto bene ad associare le $\\boldsymbol{x}$ di $\\mathcal{T}$ alle corrispondenti $\\boldsymbol{y}$, ma al prezzo di non essere capace di fare altrettanto con delle $\\boldsymbol{x}$ \"nuove\". La NN ha quindi buone performance su $\\mathcal{T}$ e cattive su $\\mathcal{P}$.\n",
    "\n",
    "Il validation set $\\mathcal{V}$, nelle NN, viene generalmente utilizzato per monitorare le ipotetiche performance del modello su $\\mathcal{P}$.\n",
    "\n",
    "I criteri di arresto principali (e più semplici) per l'addestramento di una NN sono quindi i seguenti:\n",
    "1. Fissare un numero massimo $e_{\\max}\\in\\mathbb{N}$ di epoche da eseguire;\n",
    "2. Fissare una numero massimo $p\\in\\mathbb{N}$ di epoche di tolleranza rispetto al quale posso accettare che la loss su $\\mathcal{V}$ cresca invece di diminuire. Oltre a ridurre i tempi di addestramento, questo criterio serve ad impedire di addestrare eccessivamente il modello e così rischiare di cadere in **overfitting**.\n",
    "\n",
    "---------------\n",
    "\n",
    "**OSSERVAZIONE** _(Ottimizzazione \"Classica\" e di NN)_**:**\n",
    "\n",
    "Mentre nei problemi di minimizzazione classici l'obiettivo è trovare $\\boldsymbol{x}^*$ che minimizzi globalmente (o al peggio localmente) una funzione _valutabile_, le NN sono invece tipicamente utilizzate per \"apprendere\" funzioni $\\boldsymbol{F}$ ignote o _difficilmente/onerosamente valutabili_ che caratterizzano le associazioni input-output di un numero limitato di campioni noti.\n",
    "\n",
    "La difficoltà sta quindi nel voler minimizzare l'errore sul test set (rappresentante dati \"nuovi\" ed \"ignoti\"), attraverso una procedura che sfrutta _altri_ dati (quelli del training set). Questo approccio, dettato dalla necessità, è il motivo alla base del fenomeno di overfitting e spiega la natura euristica/empirica di alcuni metodi di ottimizzazione utilizzati per addestrare le NN. Infatti, non necessariamente un minimo globale/locale per la loss su $\\mathcal{T}$ si traduce in una loss altrettanto bassa per $\\mathcal{V}$ e $\\mathcal{P}$. \n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-table",
   "metadata": {},
   "source": [
    "## MLP e Applicazioni\n",
    "\n",
    "\n",
    "Gli MLP, ed in generale le NN caratterizzate da soli FC layer, sono principalmente utilizzate per problemi di regressione/classificazione con input vettoriali non eccessivamente grandi (indicativamente $\\mathbb{R}^n$ con $n$ dell'ordine delle centinaia).\n",
    "\n",
    "Per problemi di grandi dimensioni un FC layer inizia a risultare oneroso in termini di memoria. Infatti, considerando un FC layer con stesso numero di unità $n$ del layer precedente, il numero di parametri da ottimizzare per solo questo layer è pari a $n^2 + n$.\n",
    "\n",
    "Per questo motivo gli MLP non sono mai eccessivamente profondi né hanno troppe unità per livello.\n",
    "\n",
    "Una particolare famiglia di layer utile ad aggirare queste difficoltà è quella dei _Convolutional Layers_ (utilizzati soprattutto nell'ambito della computer vision, dove la vettorializzazione di immagini significherebbe vettori con milioni di elementi).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-vietnam",
   "metadata": {},
   "source": [
    "# Esercitazione: MLP in Scikit-Learn\n",
    "\n",
    "\n",
    "In questa esercitazione applicheremo la classe _MLPClassifier_ di scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) al problema di classificazione di volti affrontato in precedenza con le SVM.\n",
    "\n",
    "Dato il non trascurabile numero di pixel per le immagini (per quanto a bassa risoluzione), applicheremo l'MLP a dei dati trasformati tramite PCA.\n",
    "\n",
    "**ATTENZIONE:** per dettagli sul dataset utilizzato, guardare le precedenti esercitazioni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interim-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** NOTA BENE! *****\n",
    "# perché %matplotlib widget funzioni, installare nell'ambiente virtuale \n",
    "# il pacchetto ipympl con il comando:\n",
    "# pip install ipympl\n",
    "#\n",
    "# ATTENZIONE: perché funzioni è necessario chiudere e rilanciare jupyter-lab\n",
    "#\n",
    "# STILE DI VISUALIZZAZIONE PLOT FATTI CON MATPLOTLIB\n",
    "%matplotlib widget\n",
    "#\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "from IPython.display import display\n",
    "\n",
    "# Il codice presente di seguito serve nel caso si verifichi un errore del tipo\n",
    "#\n",
    "# \"URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1124)>\"\n",
    "#\n",
    "# al momento di chiamare la funzione fetch_lfw_people di sklearn.datasets\n",
    "#\n",
    "# ATTENZIONE: il codice di seguito non è quindi sempre necessario; se non lo fosse, commentarlo pure.\n",
    "#\n",
    "\n",
    "import os, ssl\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-participation",
   "metadata": {},
   "source": [
    "## Importazione del Dataset e Creazione di Training, Validation e Test set\n",
    "\n",
    "Importiamo il dataset $\\mathcal{D}$ da scikit-learn e dividiamolo in $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$. Utilizzare le seguenti percentuali:\n",
    "1. $|\\mathcal{T}| = 40\\% |\\mathcal{D}|$\n",
    "1. $|\\mathcal{V}| = 10\\% |\\mathcal{D}|$\n",
    "1. $|\\mathcal{P}| = 50\\% |\\mathcal{D}|$\n",
    "\n",
    "**ATTENZIONE:** visto che la classe MLPClassifier lo esegue in automatico, _NON_ sarà necessario trasformare le classi secondo la codifica del one-hot encoding.\n",
    "\n",
    "**ESERCIZIO:** completare il codice nella cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "scheduled-annex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_trainval</th>\n",
       "      <th>X_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N. sanmples</th>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N.features</th>\n",
       "      <td>1850</td>\n",
       "      <td>1850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             X_trainval  X_test\n",
       "N. sanmples         644     644\n",
       "N.features         1850    1850"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "face_data = lfw_people['data']\n",
    "face_images = lfw_people['images']\n",
    "face_tnames = lfw_people['target_names']\n",
    "face_targets = lfw_people['target']\n",
    "\n",
    "# Creare gli X_trainval, y_trainval, X_test, y_test\n",
    "# (RICORDA: il validation set viene creato \"internamente\" dalla classe MLPClassifier. \n",
    "# Gli deve essere solamente specificata la percentuale rispetto al training set)\n",
    "\n",
    "random_state = 20210527\n",
    "test_p = ...\n",
    "val_p = ...  # Percentuale di dati di X_trainval da usare come validation set\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(..., ..., test_size=..., \n",
    "                                                          random_state=..., shuffle=True)\n",
    "\n",
    "display(pd.DataFrame({'X_trainval': X_trainval.shape, 'X_test': X_test.shape}, index=['N. sanmples', 'N.features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-seminar",
   "metadata": {},
   "source": [
    "### PCA sul Training e Validation Set\n",
    "\n",
    "Prepariamo una PCA rispettoa i dati di *X_trainval*. Non considerimo quelli di *X_test* poiché simulato dati non a disposizione.\n",
    "\n",
    "Selezionare un numero di PC che spieghino il $95\\%$ della varianza totale dei dati.\n",
    "\n",
    "**NOTA:** A voler essere precisi, ancora meglio sarebbe stato preparare la PCA solo sui dati di training, escludendo quelli di validation. Tuttavia (per quanto mi risulta) la classe _MLPClassifier_ accetta solamente la fraione di training da usare come validation, non direttamente dei dati. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "secure-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numero PC</th>\n",
       "      <th>% Varianza Tot. Spiegata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X_trainval</th>\n",
       "      <td>123</td>\n",
       "      <td>0.950428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Numero PC  % Varianza Tot. Spiegata\n",
       "X_trainval        123                  0.950428"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparazione PCA\n",
    "\n",
    "pca = ...\n",
    "\n",
    "pca.fit(...)\n",
    "\n",
    "display(pd.DataFrame({'Numero PC': pca.n_components_, \n",
    "                      '% Varianza Tot. Spiegata': pca.explained_variance_ratio_.sum()}, \n",
    "                     index=['X_trainval']))\n",
    "\n",
    "# Trasformazione dati. Salvare i vecchi in \"copie di backup\"\n",
    "\n",
    "X_trainval_old = X_trainval.copy()\n",
    "X_trainval = ...\n",
    "\n",
    "X_test_old = X_test.copy()\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-kansas",
   "metadata": {},
   "source": [
    "### Costruzione e Addestramento MLP\n",
    "\n",
    "Costruiamo un MLP caratterizzato da 5 hidden layers con 256 unità ognuno e funzione di attivazione _relu_.\n",
    "\n",
    "Come parametri di addestramento, si utilizzino quelli qui indicati:\n",
    "1. **Batch size**: 32;\n",
    "2. **Massimo numero di epoche**: 1000;\n",
    "3. **Early stopping**: pazienza di 150 epoche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "german-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione iper-parametri MLP\n",
    "hidden_layer_sizes = ...\n",
    "activation = ...\n",
    "patience = ...\n",
    "max_epochs = ...\n",
    "verbose = False\n",
    "batch_sz = ...\n",
    "\n",
    "# Inizializzazione MLP\n",
    "mlp = MLPClassifier(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "driven-ontario",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=8, early_stopping=True,\n",
       "              hidden_layer_sizes=[256, 256, 256, 256, 256], max_iter=5000,\n",
       "              n_iter_no_change=250, random_state=20210527,\n",
       "              validation_fraction=0.2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addestramento MLP\n",
    "mlp.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "saved-hopkins",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train. + val.</th>\n",
       "      <td>0.954969</td>\n",
       "      <td>0.955004</td>\n",
       "      <td>0.954969</td>\n",
       "      <td>0.954546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.729814</td>\n",
       "      <td>0.741591</td>\n",
       "      <td>0.729814</td>\n",
       "      <td>0.730685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Precision    Recall        F1\n",
       "train. + val.  0.954969   0.955004  0.954969  0.954546\n",
       "test           0.729814   0.741591  0.729814  0.730685"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon                 20             6                4              5   \n",
       "Colin Powell                  6            77                4              9   \n",
       "Donald Rumsfeld               4             3               41             11   \n",
       "George W Bush                 4             5                8            223   \n",
       "Gerhard Schroeder             0             1                1              7   \n",
       "Hugo Chavez                   0             0                0              5   \n",
       "Tony Blair                    1             0                2              7   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                       1            0           7  \n",
       "Colin Powell                       4            2           9  \n",
       "Donald Rumsfeld                    1            0           8  \n",
       "George W Bush                      5            3          12  \n",
       "Gerhard Schroeder                 32            1           8  \n",
       "Hugo Chavez                        9           24           3  \n",
       "Tony Blair                         7            1          53  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.081081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.857692</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.046154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.746479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon           0.465116      0.139535         0.093023       0.116279   \n",
       "Colin Powell           0.054054      0.693694         0.036036       0.081081   \n",
       "Donald Rumsfeld        0.058824      0.044118         0.602941       0.161765   \n",
       "George W Bush          0.015385      0.019231         0.030769       0.857692   \n",
       "Gerhard Schroeder      0.000000      0.020000         0.020000       0.140000   \n",
       "Hugo Chavez            0.000000      0.000000         0.000000       0.121951   \n",
       "Tony Blair             0.014085      0.000000         0.028169       0.098592   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                0.023256     0.000000    0.162791  \n",
       "Colin Powell                0.036036     0.018018    0.081081  \n",
       "Donald Rumsfeld             0.014706     0.000000    0.117647  \n",
       "George W Bush               0.019231     0.011538    0.046154  \n",
       "Gerhard Schroeder           0.640000     0.020000    0.160000  \n",
       "Hugo Chavez                 0.219512     0.585366    0.073171  \n",
       "Tony Blair                  0.098592     0.014085    0.746479  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.836957</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.033708</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.835206</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon           0.571429      0.065217         0.066667       0.018727   \n",
       "Colin Powell           0.171429      0.836957         0.066667       0.033708   \n",
       "Donald Rumsfeld        0.114286      0.032609         0.683333       0.041199   \n",
       "George W Bush          0.114286      0.054348         0.133333       0.835206   \n",
       "Gerhard Schroeder      0.000000      0.010870         0.016667       0.026217   \n",
       "Hugo Chavez            0.000000      0.000000         0.000000       0.018727   \n",
       "Tony Blair             0.028571      0.000000         0.033333       0.026217   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                0.016949     0.000000        0.07  \n",
       "Colin Powell                0.067797     0.064516        0.09  \n",
       "Donald Rumsfeld             0.016949     0.000000        0.08  \n",
       "George W Bush               0.084746     0.096774        0.12  \n",
       "Gerhard Schroeder           0.542373     0.032258        0.08  \n",
       "Hugo Chavez                 0.152542     0.774194        0.03  \n",
       "Tony Blair                  0.118644     0.032258        0.53  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performance\n",
    "\n",
    "y_pred_trainval = mlp.predict(X_trainval)\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "acc_trainval = mlp.score(X_trainval, y_trainval)\n",
    "prec_trainval = precision_score(y_trainval, y_pred_trainval, average='weighted')\n",
    "rec_trainval = recall_score(y_trainval, y_pred_trainval, average='weighted')\n",
    "f1_trainval = f1_score(y_trainval, y_pred_trainval, average='weighted')\n",
    "\n",
    "acc = mlp.score(X_test, y_test)\n",
    "prec = precision_score(y_test, y_pred, average='weighted')\n",
    "rec = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "df_perf = pd.DataFrame({'Accuracy': [acc_trainval, acc], \n",
    "                        'Precision': [prec_trainval, prec], \n",
    "                        'Recall': [rec_trainval, rec],\n",
    "                        'F1': [f1_trainval, f1]\n",
    "                       },\n",
    "                      index=['train. + val.', 'test'])\n",
    "\n",
    "cmat = confusion_matrix(y_test, y_pred, labels=mlp.classes_)\n",
    "cmat_norm_true = confusion_matrix(y_test, y_pred, labels=mlp.classes_, normalize='true')\n",
    "cmat_norm_pred = confusion_matrix(y_test, y_pred, labels=mlp.classes_, normalize='pred')\n",
    "\n",
    "df_cmat = pd.DataFrame(cmat, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_true = pd.DataFrame(cmat_norm_true, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_pred = pd.DataFrame(cmat_norm_pred, columns=face_tnames, index=face_tnames)\n",
    "\n",
    "display(df_perf)\n",
    "display(df_cmat)\n",
    "display(df_cmat_norm_true)\n",
    "display(df_cmat_norm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-zealand",
   "metadata": {},
   "source": [
    "## Alcuni Esempi Visivi\n",
    "\n",
    "Mostriamo visivamente come viene fatta la classificazione multi-classe. \n",
    "\n",
    "**RICORDA:** la classe MLPClassifier ha un metodo _predict_proba_ che restituisce le probabilità di appartenenza di un input ad ognuna delle classi. Quella con probabilità massima è poi quella predetta in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "center-spare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e39b9a0ccf445309c429a049948f0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48541d509e144023832ff5d9aae6832a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb14ad3c798140f89a37430c5ff4d85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6733a7ef9dc0493ebbed0b7e7eafcfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae37379a65d74f1ab14c02990ebacf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b33e269832457f886f552d47c21e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b87c01d5b94ae4b4f1344397c18f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746dde4a2bc84994ad76bc272f488e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ca25c8630247ef90060c33f3f3c337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d068611a4c04aac972bd7c30e198f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d958a42102154fd79cef883b0a270214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a85f8fac334dd789ff89df325bdad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc35ab57ff4944b2bbb508fe70e88721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a99763ebad84b85af35636f5efecae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5851ce793b457e937a77a97553a4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bef0b4d523457aa4e934efc7825e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c276db586fe4b44a8542234a40b9d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07eb47720e0e4b10aaf48d358deceee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309c75c06a9d48b0a49c6f41b899e40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b31e26f3704c75b2176561f11cdb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a15e94213c14d16a2b8774fce4ad1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c426ad85ca47e897805613a33a9bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775d014bcd7c4dd1bdef9512c010f723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e81848542ae470890ba877ab6e28351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45beef58e9fb4fe2a4cdb3f05e0c4123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Abbreviazione nomi per etichette in barplot\n",
    "face_tnames_short = []\n",
    "for name in face_tnames:\n",
    "    name_split = name.split(' ')\n",
    "    nm = ''\n",
    "    for word in name_split:\n",
    "        nm = nm + word[0]\n",
    "    face_tnames_short.append(nm)\n",
    "\n",
    "# Selezione di \"n_randsamples\" volti random dal dataset\n",
    "\n",
    "n_randsamples = 25\n",
    "ind_test_rand = np.random.choice(len(y_test), n_randsamples, replace=False)\n",
    "\n",
    "# Matrice delle n_randsamples volti scelti (una riga, un volto)\n",
    "rand_faces = X_test_old[ind_test_rand, :]\n",
    "rand_targets = y_test[ind_test_rand]\n",
    "\n",
    "# Decision Function per i volti random:\n",
    "rand_faces_decision = mlp.predict_proba(pca.transform(rand_faces))\n",
    "y_pred_rand_faces = mlp.predict(pca.transform(rand_faces))\n",
    "\n",
    "for i in range(n_randsamples):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    ii = ind_test_rand[i]\n",
    "    face_ii = face_images[ii]\n",
    "    \n",
    "    axs[0].imshow(face_ii, cmap=plt.cm.gray)\n",
    "    axs[0].set_title('Volto {} ({})'.format(ii, face_tnames[face_targets[ii]]))\n",
    "    \n",
    "    axs[1].bar(np.arange(len(face_tnames)),\n",
    "               rand_faces_decision[i, :]\n",
    "              )\n",
    "    axs[1].grid()\n",
    "    axs[1].set_xticks(np.arange(len(face_tnames)))\n",
    "    axs[1].set_xticklabels(face_tnames_short,\n",
    "                           rotation=15,\n",
    "                           fontsize=12\n",
    "                          )\n",
    "    axs[1].set_title('Predizione: {}'.format(face_tnames[y_pred_rand_faces[i]]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-trunk",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
